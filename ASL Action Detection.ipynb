{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow<2.11 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0)\n",
      "ERROR: No matching distribution found for tensorflow<2.11\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install \"tensorflow<2.11\" tensorflow tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\Users\\FRC 1771\\.conda\\envs\\tf-gpu\\python.exe \"C:\\Users\\FRC 1771\\.conda\\envs\\tf-gpu\\Scripts\\pip-script.py\" install  opencv-python mediapipe sklearn matplotlib'\n"
     ]
    }
   ],
   "source": [
    "!pip install  opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 12:21:59.537752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    if results:\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        # # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # # Draw pose connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          ) \n",
    "    # # Draw left hand connections\n",
    "    # mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "    #                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "    #                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "    #                              ) \n",
    "    # # Draw right hand connections  \n",
    "    # mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "    #                          mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "    #                          ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WIDTH = 854\n",
    "HEIGHT = 480\n",
    "def rescale_frame(frame):\n",
    "    dim = (WIDTH, HEIGHT)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1500414895.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"running 1\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# import the opencv library \n",
    "import cv2 \n",
    "  \n",
    "  \n",
    "# define a video capture object \n",
    "    print(\"running 1\")\n",
    "\n",
    "vid = cv2.VideoCapture(0) \n",
    "  \n",
    "while(True): \n",
    "    print(\"running\")\n",
    "      \n",
    "    # Capture the video frame \n",
    "    # by frame \n",
    "    ret, frame = vid.read() \n",
    "  \n",
    "    # Display the resulting frame \n",
    "    cv2.imshow('frame', frame) \n",
    "      \n",
    "    # the 'q' button is set as the \n",
    "    # quitting button you may use any \n",
    "    # desired button of your choice \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "# cap = cv2.VideoCapture(\"video.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('Y', 'U', 'Y', 'V')) # depends on fourcc available camera\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "# cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "# Set mediapipe model \n",
    "\n",
    "i = 0\n",
    "results = []\n",
    "\n",
    "with mp_hands.Hands(model_complexity=0,min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # sleep(2)\n",
    "        \n",
    "        # i += 1\n",
    "                \n",
    "        if ret == True:\n",
    "            frame = rescale_frame(frame)\n",
    "            \n",
    "            # Make detections\n",
    "            if (i % 2 == 0):\n",
    "                image, results = mediapipe_detection(frame, hands)\n",
    "                \n",
    "                frame = image\n",
    "            \n",
    "            \n",
    "                # print(results.multi_hand_landmarks)\n",
    "\n",
    "                # image = cv2.resize(image, (640,480))\n",
    "\n",
    "            # cv2.imshow('OpenCV Feed', frame)\n",
    "                \n",
    "                # Draw landmarks\n",
    "            if cv2.waitKey(5) & 0xFF == ord('p'):\n",
    "                draw_styled_landmarks(frame, results)\n",
    "\n",
    "                # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', frame)\n",
    "        else:\n",
    "            print(\"Out of frames\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results.multi_hand_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_keypoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (extract_keypoints(results))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_keypoints' is not defined"
     ]
    }
   ],
   "source": [
    "(extract_keypoints(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    lh = np.zeros(21*3)\n",
    "    if (results.multi_hand_landmarks):\n",
    "        if (results.multi_hand_landmarks[0]):\n",
    "            # print([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark])\n",
    "            min_x = 9999\n",
    "            min_y = 9999\n",
    "            min_z = 9999\n",
    "            for res in results.multi_hand_landmarks[0].landmark :\n",
    "                # print(res)\n",
    "                if (res.x < min_x):\n",
    "                    min_x = res.x\n",
    "                if (res.y < min_y):\n",
    "                    min_y = res.y\n",
    "                if (res.z < min_z):\n",
    "                    min_z = res.z\n",
    "            # print (min_x)\n",
    "            # print (min_y)\n",
    "            # print (min_z)\n",
    "            for res in results.multi_hand_landmarks[0].landmark :\n",
    "                res.x -= min_x\n",
    "                res.y -= min_y\n",
    "                res.z -= min_z\n",
    "                                           \n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten()\n",
    "            # print(lh)\n",
    "    rh = np.zeros(21*3)\n",
    "    if (results.multi_hand_landmarks):\n",
    "        if (len(results.multi_hand_landmarks) > 1):\n",
    "            min_x = 9999\n",
    "            min_y = 9999\n",
    "            min_z = 9999\n",
    "            for res in results.multi_hand_landmarks[1].landmark :\n",
    "                if (res.x < min_x):\n",
    "                    min_x = res.x\n",
    "                if (res.y < min_y):\n",
    "                    min_y = res.y\n",
    "                if (res.z < min_z):\n",
    "                    min_z = res.z\n",
    "            for res in results.multi_hand_landmarks[1].landmark :\n",
    "                res.x -= min_x\n",
    "                res.y -= min_y\n",
    "                res.z -= min_z\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[1].landmark]).flatten()\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "TRAINING_PATH = os.path.join('MS-ASL/MS-ASL/videos')\n",
    "\n",
    "# Actions that we try to detect\n",
    "# actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "actions = np.array(['eat', 'fish', 'nice', 'milk', 'teacher', 'finish', 'cousin', 'orange', 'yes', 'student', 'sister', 'friend', 'yellow',\n",
    "                   'white', 'what', 'water', 'want', 'tired', 'pencil', 'mother', 'like', 'drink', 'again', 'table', 'school', 'no', 'help', 'blue', 'spring',\n",
    "                   'doctor', 'deaf', 'red', 'father', 'black'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat' 'fish' 'nice' 'milk' 'teacher' 'finish' 'cousin' 'orange' 'yes'\n",
      " 'student' 'sister' 'friend' 'yellow' 'white' 'what' 'water' 'want'\n",
      " 'tired' 'pencil' 'mother' 'like' 'drink' 'again' 'table' 'school' 'no'\n",
      " 'help' 'blue' 'spring' 'doctor' 'deaf' 'red' 'father' 'black']\n"
     ]
    }
   ],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    dir = os.path.join(DATA_PATH, action);\n",
    "    dirmax = np.max(np.array(os.listdir(dir)).astype(int)) if os.path.exists(dir) else 0\n",
    "    for sequence in range(1,no_sequences+1):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    dir = os.path.join(TRAINING_PATH, action);\n",
    "    dirmax = np.max(np.array(os.listdir(dir)).astype(int)) if os.path.exists(dir) else 0\n",
    "    for sequence in range(1,no_sequences+1):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(TRAINING_PATH, action, str(dirmax+sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('Y', 'U', 'Y', 'V')) # depends on fourcc available camera\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "# cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "# Set mediapipe model \n",
    "with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        dir = os.path.join(TRAINING_PATH, action)\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, no_sequences+1):\n",
    "            dir2 = os.path.join(dir, str(sequence) + \".mp4\")\n",
    "            print(dir2)\n",
    "            # print(dir)\n",
    "            cap = cv2.VideoCapture(dir2)\n",
    "            \n",
    "            # print(sequence)\n",
    "            \n",
    "            # Loop through video length aka sequence length\n",
    "            frame_num = 0\n",
    "            \n",
    "            keypoints = []\n",
    "            \n",
    "            while(frame_num < sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # print(frame_num)\n",
    "                \n",
    "                if (ret != True):\n",
    "                    while (frame_num < sequence_length):\n",
    "                        frame_num += 1\n",
    "                        npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                    \n",
    "                    print(\"Finished!\")\n",
    "                    continue\n",
    "                \n",
    "                frame = rescale_frame(frame)\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, hands)\n",
    "                \n",
    "                if (results.multi_hand_landmarks):\n",
    "                    \n",
    "                    frame_num += 1\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "\n",
    "                    # NEW Apply wait logic\n",
    "                    # if frame_num == 0: \n",
    "                    #     cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                    #                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    #     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                    #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    #     # Show to screen\n",
    "                    #     cv2.imshow('OpenCV Feed', image)\n",
    "                    #     cv2.waitKey(1500)\n",
    "                    # else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                    # NEW Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.07372111 0.02786348 0.01911409 0.04260802 0.02333661\n",
      " 0.04519677 0.02271098 0.0198087  0.06500793 0.01091886 0.01641507\n",
      " 0.07992423 0.         0.01293457 0.04438096 0.03518683 0.01643586\n",
      " 0.05982426 0.0517444  0.00887192 0.07116848 0.06419826 0.00384299\n",
      " 0.0786739  0.07364947 0.00048594 0.03923407 0.06256866 0.01440729\n",
      " 0.05462161 0.10239798 0.00673579 0.06000218 0.12512761 0.00287807\n",
      " 0.06267467 0.1398083  0.         0.03244993 0.09117419 0.01239146\n",
      " 0.04551256 0.13053614 0.00378261 0.05208439 0.15226036 0.00264808\n",
      " 0.05645078 0.1684438  0.00263579 0.02407584 0.11716795 0.0103071\n",
      " 0.03512727 0.15187222 0.00297833 0.04194069 0.17209792 0.00264455\n",
      " 0.04676357 0.18928587 0.00345193 0.         0.05991817 0.02198441\n",
      " 0.01883683 0.0247705  0.01591028 0.04160371 0.01063609 0.01020616\n",
      " 0.05929461 0.00819993 0.00474914 0.07230619 0.         0.\n",
      " 0.04760987 0.07233894 0.01147038 0.06071568 0.10323721 0.00692978\n",
      " 0.05536631 0.09221733 0.00431742 0.04631358 0.07898188 0.0032026\n",
      " 0.03905454 0.10119838 0.01126787 0.05344674 0.1303339  0.00882402\n",
      " 0.04884499 0.11823791 0.00834544 0.03958568 0.10512263 0.00762346\n",
      " 0.03103396 0.12476122 0.01075826 0.04508355 0.15177262 0.00989202\n",
      " 0.04081231 0.13991898 0.0119979  0.03287169 0.12823898 0.01259154\n",
      " 0.02343611 0.14353067 0.00965472 0.03445452 0.16722375 0.01016239\n",
      " 0.03076816 0.15738684 0.01323043 0.02452341 0.14745128 0.01511268]\n"
     ]
    }
   ],
   "source": [
    "print(np.load(os.path.join(DATA_PATH, \"fish\", str(30), \"{}.npy\".format(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "fish\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "nice\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "milk\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "teacher\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "finish\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "cousin\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "orange\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "yes\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "student\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "sister\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "friend\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "yellow\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "white\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "what\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "water\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "want\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "tired\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "pencil\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "mother\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "like\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "drink\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "again\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "table\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "school\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "no\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "help\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "blue\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "spring\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "doctor\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "deaf\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "red\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "father\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "black\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    print(action)\n",
    "    for sequence in range(start_folder, no_sequences + 1):\n",
    "        window = []\n",
    "        # dir = os.path.join(DATA_PATH, action, str(sequence))\n",
    "        # arr = np.array(os.listdir(dir))\n",
    "        # arr[:] = [s[:-4] for s in arr]\n",
    "        # dirmax = np.max(arr.astype(int)) if arr.size != 0 else 0\n",
    "        print(sequence)\n",
    "        for frame_num in range(1, sequence_length + 1):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            if not (res.any()):\n",
    "                print(action);\n",
    "                print(sequence)\n",
    "            window.append(res)\n",
    "            # print(frame_num)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range (1, len(sequences)):\n",
    "    if (len(sequences[i][1]) != 126):\n",
    "        print(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "NUM_PARALLEL_EXEC_UNITS=6\n",
    "\n",
    "#Assume that the number of cores per socket in the machine is denoted as NUM_PARALLEL_EXEC_UNITS\n",
    "#  when NUM_PARALLEL_EXEC_UNITS=0 the system chooses appropriate settings \n",
    "\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=NUM_PARALLEL_EXEC_UNITS, \n",
    "                        inter_op_parallelism_thrzeads=2, \n",
    "                        allow_soft_placement=True,\n",
    "                        device_count = {'CPU': NUM_PARALLEL_EXEC_UNITS})\n",
    "\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(6)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 12:22:37.407032: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(np.array(sequences[0]).shape)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=10000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['categorical_accuracy'], color='orange', label='categorical_accuracy')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.argmax(res[2])|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('msasl1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_weights('msasl1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[48,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[48,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[48,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[47,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[47,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 1,  0]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[47,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[49,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[50,  1],\n",
       "        [ 0,  0]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803921568627451"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245), (16,117,245), (16,117,245)] * 10\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "predictions = []\n",
    "res = []\n",
    "sequence = []\n",
    "threshold = 0.85\n",
    "\n",
    "import time\n",
    "\n",
    "predict = True\n",
    "\n",
    "def predict_word() :\n",
    "    global res\n",
    "    while predict :\n",
    "        time.sleep(0.5)\n",
    "        if (len(sequence) == sequence_length) :\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=None)[0]\n",
    "            # print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "        else:\n",
    "            res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame = np.zeros(1)\n",
    "run_cv_thread = True\n",
    "def run_cv():\n",
    "    global frame\n",
    "    cap = cv2.VideoCapture(\"Validation/Cool/1.mkv\")\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    while run_cv_thread:\n",
    "        ret, frame_new = cap.read()\n",
    "        time.sleep(1 / 15.0)\n",
    "        if (ret == False):\n",
    "            break\n",
    "        frame = frame_new\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame = np.zeros(1)\n",
    "run_media = True\n",
    "\n",
    "def run_mediapipe() :\n",
    "    with mp_hands.Hands(model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "        global frame\n",
    "        predictions = []\n",
    "        res = []\n",
    "        sentence = []\n",
    "        frames_wo_points = 0\n",
    "        while run_media :\n",
    "            time.sleep(1 / 15.0)\n",
    "            if (frame.any()):\n",
    "                frame = rescale_frame(frame)\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, hands)\n",
    "                # print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # 2. Prediction logic\n",
    "                keypoints = extract_keypoints(results)\n",
    "#                 if np.any(keypoints):\n",
    "#                     frames_wo_points = 0\n",
    "#                     sequence.append(keypoints)\n",
    "#                     sequence = sequence[-sequence_length:]\n",
    "#                 else:\n",
    "#                     frames_wo_points += 1\n",
    "#                     if (frames_wo_points >= 10):\n",
    "#                         sequence = []\n",
    "\n",
    "#                 #3. Viz logic\n",
    "#                 # print(np.any(predictions) & np.any(res))\n",
    "#                 if (np.any(predictions) & np.any(res)) :\n",
    "#                     if np.unique(predictions[-3:])[0]==np.argmax(res): \n",
    "#                         # print(res[np.argmax(res)])\n",
    "#                         # print(np.argmax(res))\n",
    "#                         if res[np.argmax(res)] > threshold: \n",
    "#                             if len(sentence) > 0: \n",
    "#                                 if actions[np.argmax(res)] != sentence[-1]:\n",
    "#                                     sentence.append(actions[np.argmax(res)])\n",
    "#                             else:\n",
    "#                                 sentence.append(actions[np.argmax(res)])\n",
    "#                             # print(sentence)\n",
    "\n",
    "#                     if len(sentence) > 5: \n",
    "#                         sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "#                     # Viz probabilities\n",
    "#                     image = prob_viz(res, actions, image, colors)\n",
    "#                 if (np.any(predictions)):\n",
    "#                     cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#                     cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Show to screenP\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                cv2.waitKey(1)\n",
    "                # cv2.destroyAllWindows()\n",
    "                # if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                #     return\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[186 196 200]\n",
      "  [186 196 200]\n",
      "  [190 196 205]\n",
      "  ...\n",
      "  [ 60  73  76]\n",
      "  [ 54  71  75]\n",
      "  [ 52  69  73]]\n",
      "\n",
      " [[185 195 199]\n",
      "  [185 195 199]\n",
      "  [188 194 203]\n",
      "  ...\n",
      "  [ 60  73  76]\n",
      "  [ 54  71  75]\n",
      "  [ 52  69  73]]\n",
      "\n",
      " [[191 194 199]\n",
      "  [191 194 199]\n",
      "  [192 195 200]\n",
      "  ...\n",
      "  [ 56  74  76]\n",
      "  [ 50  71  75]\n",
      "  [ 48  69  73]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[199 214 225]\n",
      "  [199 214 225]\n",
      "  [196 213 224]\n",
      "  ...\n",
      "  [ 19   9  16]\n",
      "  [ 25  10  19]\n",
      "  [ 26  11  20]]\n",
      "\n",
      " [[196 213 224]\n",
      "  [196 213 224]\n",
      "  [195 212 223]\n",
      "  ...\n",
      "  [ 18   8  15]\n",
      "  [ 21   8  17]\n",
      "  [ 23  10  19]]\n",
      "\n",
      " [[196 213 224]\n",
      "  [196 213 224]\n",
      "  [195 212 223]\n",
      "  ...\n",
      "  [ 18   8  15]\n",
      "  [ 21   8  17]\n",
      "  [ 21   8  17]]]\n"
     ]
    }
   ],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "t1 = threading.Thread(target=predict_word, args=())\n",
    "t1.start()\n",
    "t2 = threading.Thread(target=run_cv, args=())\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir = os.path.join(\"MS-ASL/MS-ASL/videos/pencil/1.mp4\")\n",
    "# dir = os.path.join(\"TRAINING\", \"hot\", \"2.mp4\")\n",
    "# cap = cv2.VideoCapture(dir)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "       \n",
    "# cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('Y', 'U', 'Y', 'V')) # depends on fourcc available camera\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)\n",
    "# cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "# Set mediapipe model \n",
    "\n",
    "frames_wo_points = 0\n",
    "\n",
    "# def run_cv(sequence) :\n",
    "with mp_hands.Hands(model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    predictions = []\n",
    "    res = []\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    while (frame.any()):\n",
    "\n",
    "        frame = rescale_frame(frame)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        # print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        if np.any(keypoints):\n",
    "            frames_wo_points = 0\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-sequence_length:]\n",
    "        else:\n",
    "            frames_wo_points += 1\n",
    "            if (frames_wo_points >= 10):\n",
    "                sequence = []\n",
    "\n",
    "        #3. Viz logic\n",
    "        # print(np.any(predictions) & np.any(res))\n",
    "        if (np.any(predictions) & np.any(res)) :\n",
    "            if np.unique(predictions[-3:])[0]==np.argmax(res): \n",
    "                # print(res[np.argmax(res)])\n",
    "                # print(np.argmax(res))\n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                    # print(sentence)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "            \n",
    "            \n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        if (np.any(predictions)):\n",
    "            cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screenP\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "       \n",
    "    # cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num, prob in enumerate(res):\n",
    "    print(num)\n",
    "    print(prob)\n",
    "    print((0,60+num*40))\n",
    "    print((int(prob*100), 90+num*40))\n",
    "    print(colors[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict = False\n",
    "t1.join()\n",
    "predict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_cv_thread = False\n",
    "t2.join()\n",
    "run_cv_thread = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(predictions[-10:])[0]==np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = np.zeros(1)\n",
    "run_process = True\n",
    "run_display = True\n",
    "def process():\n",
    "    global q\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    while ret & run_process:\n",
    "        ret, frame = cap.read()\n",
    "#detection part in my case I use tensorflow then \n",
    "     # end of detection part \n",
    "        q = frame\n",
    "    cap.release()\n",
    "\n",
    "def Display():\n",
    "    while run_display:\n",
    "        if q.any():\n",
    "            frame = q\n",
    "            cv2.imshow(\"frame1\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == '__main__':?\n",
    "#  start threads\n",
    "p1 = threading.Thread(target=process)\n",
    "p2 = threading.Thread(target=Display)\n",
    "p1.start()\n",
    "p2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while run_display:\n",
    "    if q.any():\n",
    "        frame = q\n",
    "        cv2.imshow(\"frame1\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_process = False\n",
    "p1.join()\n",
    "run_process = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 12. Improve Performance using Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -U tensorboard_plugin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def video_to_frames(video_path, size=None):\n",
    "    # \"\"\"\n",
    "    # video_path -> str, path to video.\n",
    "    # size -> (int, int), width, height.\n",
    "    # \"\"\"\n",
    "\n",
    "cap = cv2.VideoCapture(\"jQb9NL9_S6U.mp4\")\n",
    "\n",
    "\n",
    "frames = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if (ret == True):\n",
    "\n",
    "        frame = rescale_frame(frame)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "            # Break gracefully\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_to_frames(\"jQb9NL9_S6U.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"jQb9NL9_S6U.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    # Read feed\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # sleep(2)\n",
    "\n",
    "    # i += 1\n",
    "\n",
    "    if ret == True:\n",
    "        frame = rescale_frame(frame)\n",
    "\n",
    "        # Make detections\n",
    "#         if (i % 2 == 0):\n",
    "#             image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "#             frame = image\n",
    "\n",
    "\n",
    "            # print(results.multi_hand_landmarks)\n",
    "\n",
    "            # image = cv2.resize(image, (640,480))\n",
    "\n",
    "        # cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "            # Draw landmarks\n",
    "        # if cv2.waitKey(5) & 0xFF == ord('p'):\n",
    "        #     draw_styled_landmarks(frame, results)\n",
    "\n",
    "            # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "    else:\n",
    "        print(\"Out of frames\")\n",
    "        break\n",
    "\n",
    "\n",
    "    # Break gracefully\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-tensor",
   "language": "python",
   "name": "conda-tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
